{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install requests beautifulsoup4 pandas\n",
        "!pip install beautifulsoup4 requests pandas"
      ],
      "metadata": {
        "id": "mCcMDMvej2oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fs8xSEJ9jOaJ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Define the base URL of the website\n",
        "base_url = 'https://dailyhindustanexpress.com/'\n",
        "\n",
        "# Initialize lists to store data\n",
        "titles = []\n",
        "urls = []\n",
        "contents = []\n",
        "\n",
        "# Define a function to fetch articles from a page\n",
        "def fetch_articles_from_page(page_url):\n",
        "    response = requests.get(page_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all articles on the current page\n",
        "    for article in soup.find_all('article'):\n",
        "        # Extract the title and URL\n",
        "        title_tag = article.find('h3', {'class': 'jeg_post_title'})  # Adjust the tag as necessary\n",
        "        if title_tag:\n",
        "            title = title_tag.get_text(strip=True)\n",
        "            url = title_tag.find('a')['href']\n",
        "\n",
        "            # Fetch the article content\n",
        "            article_response = requests.get(url)\n",
        "            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "            # Extract the content\n",
        "            content_tag = article_soup.find('div', {'class': 'content-inner'})  # Adjust the class as necessary\n",
        "            content = content_tag.get_text(strip=True) if content_tag else 'No content found'\n",
        "\n",
        "            # Append the data to lists\n",
        "            titles.append(title)\n",
        "            urls.append(url)\n",
        "            contents.append(content)\n",
        "\n",
        "# Fetch the first page\n",
        "current_page_url = base_url\n",
        "\n",
        "# Loop through all pages using the \"next\" button\n",
        "while current_page_url:\n",
        "    # Fetch articles from the current page\n",
        "    fetch_articles_from_page(current_page_url)\n",
        "\n",
        "    # Fetch the next page URL\n",
        "    response = requests.get(current_page_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    next_page_tag = soup.find('a', {'class': 'page_nav next'})  # Adjust the class based on the inspect code\n",
        "    if next_page_tag:\n",
        "        next_page_url = next_page_tag['href']\n",
        "        current_page_url = next_page_url  # Update the URL for the next page\n",
        "    else:\n",
        "        current_page_url = None  # Stop the loop if there is no next page\n",
        "\n",
        "# Create a DataFrame from the lists\n",
        "df = pd.DataFrame({\n",
        "    'Title': titles,\n",
        "    'URL': urls,\n",
        "    'Content': contents\n",
        "})\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv('HindustanExpress.csv', index=False)\n",
        "print(\"Scraping completed\")"
      ]
    }
  ]
}