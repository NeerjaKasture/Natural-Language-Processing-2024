{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG70sw73mrZA"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Initialize Chrome driver (in headless mode for faster scraping)\n",
        "try:\n",
        "    driver = webdriver.Chrome()\n",
        "except:\n",
        "    from selenium.webdriver.chrome.options import Options\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    print(\"Running in headless mode.\")\n",
        "\n",
        "# URL of the page containing the desired links\n",
        "url = \"https://urdu.news18.com/sports/\"\n",
        "driver.get(url)\n",
        "\n",
        "# Wait for the page to load\n",
        "wait = WebDriverWait(driver, 10)\n",
        "\n",
        "# Open the CSV file in append mode\n",
        "with open(r\"C:\\Users\\anura\\Desktop\\NLP\\Assignment_1\\urls.csv\", mode=\"a\", newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Infinite loop to keep clicking 'Load More' button until it disappears\n",
        "    while True:\n",
        "        try:\n",
        "            # Find all list items containing links\n",
        "            list_items = driver.find_elements(By.CSS_SELECTOR, 'li.jsx-bcef60bca6506e85 a')\n",
        "\n",
        "            for item in list_items:\n",
        "                link = item.get_attribute('href')\n",
        "                if link and \"/news\" in link:  # Filter for links containing \"/news\"\n",
        "                    full_link = link if link.startswith(\"http\") else \"https://urdu.news18.com\" + link\n",
        "                    print(f\"Found URL: {full_link}\")\n",
        "                    writer.writerow([full_link])  # Append the URL to the CSV file\n",
        "\n",
        "            # Scroll down to load more content (click on 'Load More' button if present)\n",
        "            load_more_button = wait.until(\n",
        "                EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.jsx-2498664463.load_more'))\n",
        "            )\n",
        "            load_more_button.click()  # Click the 'Load More' button\n",
        "            time.sleep(2)  # Wait for new content to load\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Finished loading or encountered an error: {str(e)}\")\n",
        "            break  # Exit the loop when there is no 'Load More' button or an error occurs\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()\n",
        "\n",
        "print(\"Scraping complete. URLs saved to urls.csv.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Initialize Chrome driver (headless mode for faster scraping)\n",
        "try:\n",
        "    driver = webdriver.Chrome()\n",
        "except:\n",
        "    from selenium.webdriver.chrome.options import Options\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    print(\"Running in headless mode.\")\n",
        "\n",
        "# Open the CSV file containing URLs (urls.csv)\n",
        "with open(r\"C:\\Users\\anura\\Desktop\\NLP\\Assignment_1\\news_18_education_urls.csv\", \"r\", newline='', encoding='utf-8') as input_file:\n",
        "    urls_reader = csv.reader(input_file)\n",
        "\n",
        "    # Open a new CSV file to store extracted articles (news18_sports.csv)\n",
        "    with open(r\"C:\\Users\\anura\\Desktop\\NLP\\Assignment_1\\news18_education.csv\", mode=\"a\", newline='', encoding='utf-8') as output_file:\n",
        "        writer = csv.writer(output_file)\n",
        "\n",
        "        # Write the header row (URL, Heading, Content)\n",
        "        writer.writerow([\"URL\", \"Heading\", \"Content\"])\n",
        "\n",
        "        # Iterate over each URL in the input CSV\n",
        "        for row in urls_reader:\n",
        "            url = row[0]  # Get the URL from the first column\n",
        "            driver.get(url)\n",
        "\n",
        "            try:\n",
        "                # Wait for the heading to load and extract the heading text\n",
        "                heading = WebDriverWait(driver, 5).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#target-div > h1\"))\n",
        "                ).text\n",
        "\n",
        "                # Keep clicking \"Read More\" until it's no longer available\n",
        "                while True:\n",
        "                    try:\n",
        "                        read_more_button = WebDriverWait(driver,1).until(\n",
        "                            EC.element_to_be_clickable((By.ID, \"readmorebox\"))\n",
        "                        )\n",
        "                        read_more_button.click()\n",
        "                        time.sleep(1)  # Wait for the content to load\n",
        "                    except:\n",
        "                        break  # Exit loop when no more \"Read More\" button\n",
        "\n",
        "                # Extract all paragraphs inside the main content section\n",
        "                paragraphs = driver.find_elements(By.CSS_SELECTOR, \"#main-content p\")\n",
        "                content = \"\\n\".join([p.text for p in paragraphs])  # Join all paragraphs into one string\n",
        "\n",
        "                # Write the URL, heading, and full content to the CSV file\n",
        "                writer.writerow([url, heading, content])\n",
        "                print(f\"Article saved: {url}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {url}: {str(e)}\")\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()\n",
        "\n",
        "print(\"Article extraction complete. Data saved to news18_sports.csv.\")\n"
      ],
      "metadata": {
        "id": "TVpHAFtFm0G0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}