{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpjeDK6WhcYv",
        "outputId": "413df245-79e8-4530-9bf6-e0baa958f9d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping completed\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_articles_from_page(page_number):\n",
        "    base_url = f'https://www.express.pk/science/archives/?page={page_number}'\n",
        "    response = requests.get(base_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    titles = []\n",
        "    urls = []\n",
        "    contents = []\n",
        "\n",
        "    # Find all article containers on the page\n",
        "    article_containers = soup.find_all('div', {'class': 'story'})\n",
        "\n",
        "    # Debug to check the containers found\n",
        "    if not article_containers:\n",
        "        print(f\"No articles found on page {page_number}\")\n",
        "\n",
        "    for container in article_containers:\n",
        "        # Find all 'a' tags with the class 'image' within the article container\n",
        "        a_tags = container.find_all('a', href=True, class_='image')\n",
        "\n",
        "        for a_tag in a_tags:\n",
        "            # Extract the article URL from the 'href' attribute\n",
        "            article_url = a_tag['href']\n",
        "\n",
        "            # Fetch the article content\n",
        "            article_response = requests.get(article_url)\n",
        "            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "            # Extract the title\n",
        "            title_tag = article_soup.find('h1', class_='title')\n",
        "            title = title_tag.get_text(strip=True) if title_tag else 'No title found'\n",
        "\n",
        "            # Extract the content\n",
        "            content_div = article_soup.find('div', class_='span-16 story-content last mobile-story-content fix-l-r')\n",
        "            content = ''\n",
        "            if content_div:\n",
        "                content = ' '.join(p.get_text(strip=True) for p in content_div.find_all('p'))\n",
        "\n",
        "            # Append extracted information to lists\n",
        "            titles.append(title)\n",
        "            urls.append(article_url)\n",
        "            contents.append(content)\n",
        "\n",
        "    return titles, urls, contents\n",
        "\n",
        "# Function to scrape articles from multiple pages\n",
        "def scrape_articles(max_pages):\n",
        "    all_titles = []\n",
        "    all_urls = []\n",
        "    all_contents = []\n",
        "\n",
        "    for page_number in range(1, max_pages + 1):\n",
        "        titles, urls, contents = scrape_articles_from_page(page_number)\n",
        "        all_titles.extend(titles)\n",
        "        all_urls.extend(urls)\n",
        "        all_contents.extend(contents)\n",
        "\n",
        "    return all_titles, all_urls, all_contents\n",
        "\n",
        "# Scrape articles from the first 10 pages (adjust as needed)\n",
        "max_pages = 643\n",
        "all_titles, all_urls, all_contents = scrape_articles(max_pages)\n",
        "\n",
        "# Create a DataFrame from the lists\n",
        "df = pd.DataFrame({\n",
        "    'Title': all_titles,\n",
        "    'URL': all_urls,\n",
        "    'Content': all_contents\n",
        "})\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv('express_science_articles.csv', index=False)\n",
        "print(\"Scraping completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from datasets import load_dataset, Dataset\n",
        "from bs4 import BeautifulSoup\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "\n",
        "\n",
        "\n",
        "repo_name = \"snehagautam/nlp_webscraping\"\n",
        "csv_file_path = \"/content/jang_archive_articles_02-08-2016 to 01-08-2017.csv\"\n",
        "\n",
        "existing_dataset = load_dataset(repo_name, split='train')\n",
        "existing_df = existing_dataset.to_pandas().reset_index(drop=True)\n",
        "new_df = pd.read_csv(csv_file_path)\n",
        "new_df = new_df.rename(columns={\n",
        "    'Title': 'heading',\n",
        "    'URL': 'url',\n",
        "    'Content': 'content'\n",
        "})\n",
        "\n",
        "combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
        "\n",
        "if '__index_level_0__' in combined_df.columns:\n",
        "    combined_df = combined_df.drop(columns=['__index_level_0__'])\n",
        "\n",
        "combined_dataset = Dataset.from_pandas(combined_df, preserve_index=False)\n",
        "\n",
        "api = HfApi()\n",
        "HfFolder.save_token(\"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\")\n",
        "combined_dataset.push_to_hub(repo_name)\n"
      ],
      "metadata": {
        "id": "ednciRyQpG01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "\n",
        "# Hugging Face repo name and CSV file path\n",
        "repo_name = \"snehagautam/nlp_webscraping\"\n",
        "csv_file_path = \"/content/jang_archive_articles_02-08-2016 to 01-08-2017.csv\"\n",
        "\n",
        "# Load the existing dataset from the Hugging Face repository\n",
        "existing_dataset = load_dataset(repo_name, split='train')\n",
        "existing_df = existing_dataset.to_pandas().reset_index(drop=True)\n",
        "\n",
        "# Process the large CSV in chunks to avoid memory issues\n",
        "chunk_size = 10000  # Define a chunk size for processing\n",
        "new_df_chunks = pd.read_csv(csv_file_path, chunksize=chunk_size)\n",
        "new_df = pd.DataFrame()\n",
        "\n",
        "for chunk in new_df_chunks:\n",
        "    # Rename columns to match the existing dataset\n",
        "    chunk = chunk.rename(columns={'Title': 'heading', 'URL': 'url', 'Content': 'content'})\n",
        "\n",
        "    # Append each chunk to the main new_df DataFrame\n",
        "    new_df = pd.concat([new_df, chunk], ignore_index=True)\n",
        "\n",
        "# Combine the old and new datasets\n",
        "combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
        "\n",
        "# Drop unnecessary index column if present\n",
        "if '__index_level_0__' in combined_df.columns:\n",
        "    combined_df = combined_df.drop(columns=['__index_level_0__'])\n",
        "\n",
        "# Convert the combined DataFrame to a Hugging Face Dataset\n",
        "combined_dataset = DatasetDict({\n",
        "    'train': Dataset.from_pandas(combined_df, preserve_index=False)\n",
        "})\n",
        "\n",
        "# Hugging Face API setup\n",
        "api = HfApi()\n",
        "HfFolder.save_token(\"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\")\n",
        "\n",
        "# Push the combined dataset to the Hugging Face Hub\n",
        "combined_dataset.push_to_hub(repo_name)\n",
        "\n",
        "print(\"Dataset push complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2kkOa-oxBHR",
        "outputId": "56b99794-7bd6-44e5-de74-bca5af059ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK5yMEXhiuE5"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_articles_from_page(page_number):\n",
        "    base_url = f'https://www.express.pk/crime/archives/?page={page_number}'\n",
        "    response = requests.get(base_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    titles = []\n",
        "    urls = []\n",
        "    contents = []\n",
        "\n",
        "    # Find all article containers on the page\n",
        "    article_containers = soup.find_all('div', {'class': 'story'})\n",
        "\n",
        "    # Debug to check the containers found\n",
        "    if not article_containers:\n",
        "        print(f\"No articles found on page {page_number}\")\n",
        "\n",
        "    for container in article_containers:\n",
        "        # Find all 'a' tags with the class 'image' within the article container\n",
        "        a_tags = container.find_all('a', href=True, class_='image')\n",
        "\n",
        "        for a_tag in a_tags:\n",
        "            # Extract the article URL from the 'href' attribute\n",
        "            article_url = a_tag['href']\n",
        "\n",
        "            # Fetch the article content\n",
        "            article_response = requests.get(article_url)\n",
        "            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "            # Extract the title\n",
        "            title_tag = article_soup.find('h1', class_='title')\n",
        "            title = title_tag.get_text(strip=True) if title_tag else 'No title found'\n",
        "\n",
        "            # Extract the content\n",
        "            content_div = article_soup.find('div', class_='span-16 story-content last mobile-story-content fix-l-r')\n",
        "            content = ''\n",
        "            if content_div:\n",
        "                content = ' '.join(p.get_text(strip=True) for p in content_div.find_all('p'))\n",
        "\n",
        "            # Append extracted information to lists\n",
        "            titles.append(title)\n",
        "            urls.append(article_url)\n",
        "            contents.append(content)\n",
        "\n",
        "    return titles, urls, contents\n",
        "\n",
        "# Function to scrape articles from multiple pages\n",
        "def scrape_articles(max_pages):\n",
        "    all_titles = []\n",
        "    all_urls = []\n",
        "    all_contents = []\n",
        "\n",
        "    for page_number in range(1, max_pages + 1):\n",
        "        titles, urls, contents = scrape_articles_from_page(page_number)\n",
        "        all_titles.extend(titles)\n",
        "        all_urls.extend(urls)\n",
        "        all_contents.extend(contents)\n",
        "\n",
        "    return all_titles, all_urls, all_contents\n",
        "\n",
        "# Scrape articles from the first 10 pages (adjust as needed)\n",
        "max_pages = 280\n",
        "all_titles, all_urls, all_contents = scrape_articles(max_pages)\n",
        "\n",
        "# Create a DataFrame from the lists\n",
        "df = pd.DataFrame({\n",
        "    'Title': all_titles,\n",
        "    'URL': all_urls,\n",
        "    'Content': all_contents\n",
        "})\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv('express_crime_articles.csv', index=False)\n",
        "print(\"Scraping completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrDc4BkGyPld"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_articles_from_page(page_number):\n",
        "    base_url = f'https://www.express.pk/business/archives/?page={page_number}'\n",
        "    response = requests.get(base_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    titles = []\n",
        "    urls = []\n",
        "    contents = []\n",
        "\n",
        "    # Find all article containers on the page\n",
        "    article_containers = soup.find_all('div', {'class': 'story'})\n",
        "\n",
        "    # Debug to check the containers found\n",
        "    if not article_containers:\n",
        "        print(f\"No articles found on page {page_number}\")\n",
        "\n",
        "    for container in article_containers:\n",
        "        # Find all 'a' tags with the class 'image' within the article container\n",
        "        a_tags = container.find_all('a', href=True, class_='image')\n",
        "\n",
        "        for a_tag in a_tags:\n",
        "            # Extract the article URL from the 'href' attribute\n",
        "            article_url = a_tag['href']\n",
        "\n",
        "            # Fetch the article content\n",
        "            article_response = requests.get(article_url)\n",
        "            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "            # Extract the title\n",
        "            title_tag = article_soup.find('h1', class_='title')\n",
        "            title = title_tag.get_text(strip=True) if title_tag else 'No title found'\n",
        "\n",
        "            # Extract the content\n",
        "            content_div = article_soup.find('div', class_='span-16 story-content last mobile-story-content fix-l-r')\n",
        "            content = ''\n",
        "            if content_div:\n",
        "                content = ' '.join(p.get_text(strip=True) for p in content_div.find_all('p'))\n",
        "\n",
        "            # Append extracted information to lists\n",
        "            titles.append(title)\n",
        "            urls.append(article_url)\n",
        "            contents.append(content)\n",
        "\n",
        "    return titles, urls, contents\n",
        "\n",
        "# Function to scrape articles from multiple pages\n",
        "def scrape_articles(max_pages):\n",
        "    all_titles = []\n",
        "    all_urls = []\n",
        "    all_contents = []\n",
        "\n",
        "    for page_number in range(1, max_pages + 1):\n",
        "        titles, urls, contents = scrape_articles_from_page(page_number)\n",
        "        all_titles.extend(titles)\n",
        "        all_urls.extend(urls)\n",
        "        all_contents.extend(contents)\n",
        "\n",
        "    return all_titles, all_urls, all_contents\n",
        "\n",
        "# Scrape articles from the first 10 pages (adjust as needed)\n",
        "max_pages = 2114\n",
        "all_titles, all_urls, all_contents = scrape_articles(max_pages)\n",
        "\n",
        "# Create a DataFrame from the lists\n",
        "df = pd.DataFrame({\n",
        "    'Title': all_titles,\n",
        "    'URL': all_urls,\n",
        "    'Content': all_contents\n",
        "})\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv('express_business_articles.csv', index=False)\n",
        "print(\"Scraping completed\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}