{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lWcvK_D09wv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, GemmaConfig, AutoTokenizer, AutoModel, MistralConfig, MistralModel, MistralForCausalLM, LlamaConfig, LlamaForCausalLM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjTDfg581F2M",
        "outputId": "6a4ea2b0-05a6-44ff-ed67-0a3220b13ab1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaConfig {\n",
              "  \"attention_bias\": false,\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bos_token_id\": 1,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"head_dim\": 64,\n",
              "  \"hidden_act\": \"silu\",\n",
              "  \"hidden_size\": 256,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 688,\n",
              "  \"max_position_embeddings\": 64,\n",
              "  \"mlp_bias\": false,\n",
              "  \"model_type\": \"llama\",\n",
              "  \"num_attention_heads\": 4,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"num_key_value_heads\": 2,\n",
              "  \"pretraining_tp\": 1,\n",
              "  \"rms_norm_eps\": 1e-06,\n",
              "  \"rope_scaling\": null,\n",
              "  \"rope_theta\": 10000.0,\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"transformers_version\": \"4.46.2\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 32769\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"NeerjaK/Urdu_Model\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "config = LlamaConfig(hidden_size=256,\n",
        "                     vocab_size=len(tokenizer.vocab),\n",
        "                     num_attention_heads=4,\n",
        "                     num_key_value_heads=2,\n",
        "                     num_hidden_layers=12,\n",
        "                     intermediate_size=688,\n",
        "                     eos_token_id = 2,\n",
        "                     bos_token_id = 1,\n",
        "                     max_position_embeddings=64)\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzJSFwgh1Nbo",
        "outputId": "b7bbaa3d-3789-4b3a-f9e2-8dee28382e94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('urdu_model/tokenizer_config.json',\n",
              " 'urdu_model/special_tokens_map.json',\n",
              " 'urdu_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model_mis = LlamaForCausalLM(config)\n",
        "model_mis.save_pretrained(\"urdu_model\")\n",
        "tokenizer.save_pretrained(\"urdu_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoGdTVHt1T8A"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LDfEGBa1Xfn"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\"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\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QdYLOwG1Z3c",
        "outputId": "2374244b-9d59-420c-ea13-158e2157cc0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 269321\n",
            "Eval dataset size: 9702\n"
          ]
        }
      ],
      "source": [
        "# Specify your dataset repository and list all relevant files\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "# Define your repo and data files\n",
        "repo_id = \"NeerjaK/Urdu_Model\"\n",
        "data_files = {\n",
        "    \"train\": [\n",
        "        \"bbc_dataset_token_train.parquet\",\n",
        "        \"jang_dataset_120000.parquet\",\n",
        "        \"jang_dataset_60000.parquet\",\n",
        "        \"jang_dataset_90000.parquet\",\n",
        "        \"jang_dataset_token_train.parquet\",\n",
        "        \"jang_dataset_200000.parquet\"\n",
        "    ],\n",
        "    \"test\": [\n",
        "        \"news18_dataset_token_test.parquet\",\n",
        "        \"bbc_dataset_token_test.parquet\",\n",
        "\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Step 1: Load all train datasets\n",
        "train_datasets = [\n",
        "    load_dataset(repo_id, data_files={\"train\": file}, split=\"train\")\n",
        "    for file in data_files[\"train\"]\n",
        "]\n",
        "\n",
        "# Step 2: Load all test datasets\n",
        "test_datasets = [\n",
        "    load_dataset(repo_id, data_files={\"test\": file}, split=\"test\")\n",
        "    for file in data_files[\"test\"]\n",
        "]\n",
        "\n",
        "# Step 3: Concatenate the datasets\n",
        "train_dataset = concatenate_datasets(train_datasets)\n",
        "eval_dataset = concatenate_datasets(test_datasets)\n",
        "\n",
        "# Step 4: Print info to verify\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Eval dataset size: {len(eval_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSJOhePp1cfb"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "class SaveModelCallback(TrainerCallback):\n",
        "    def __init__(self, repo_id, log_interval=0.1, log_file=\"perplexity_log.txt\"):\n",
        "        self.repo_id = repo_id\n",
        "        self.log_interval = log_interval\n",
        "        self.steps_per_interval = None\n",
        "        self.perplexity_log = []  # Store perplexity values\n",
        "        self.log_file = log_file\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        # Calculate steps per epoch\n",
        "        steps_per_epoch = state.max_steps / args.num_train_epochs\n",
        "        # Calculate steps for the specified interval (e.g., every 0.1 epoch)\n",
        "        self.steps_per_interval = int(steps_per_epoch * self.log_interval)\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        # Track perplexity at every log_interval (e.g., 0.1 epoch)\n",
        "        if self.steps_per_interval and state.global_step % self.steps_per_interval == 0:\n",
        "            loss = logs.get(\"loss\")\n",
        "            if loss is not None:\n",
        "                perplexity = math.exp(loss) if loss < 100 else float('inf')  # Prevent overflow\n",
        "                print(f\"Perplexity at step {state.global_step}: {perplexity:.4f}\")\n",
        "\n",
        "                # Save perplexity to list\n",
        "                self.perplexity_log.append((state.global_step, perplexity))\n",
        "\n",
        "    def on_save(self, args, state, control, model=None, tokenizer=None, **kwargs):\n",
        "        # Saving model and tokenizer to Hugging Face Hub\n",
        "        print(f\"Saving model checkpoint at step {state.global_step}\")\n",
        "        model.push_to_hub(commit_message=\"Saving checkpoint\", repo_id=self.repo_id)\n",
        "        # tokenizer.push_to_hub(commit_message=\"Saving tokenizer\", repo_id=self.repo_id)\n",
        "\n",
        "        # Save perplexity log to a file\n",
        "        if self.perplexity_log:\n",
        "            with open(self.log_file, \"a\") as f:\n",
        "                for step, perplexity in self.perplexity_log:\n",
        "                    f.write(f\"Step {step}: Perplexity {perplexity:.4f}\\n\")\n",
        "            self.perplexity_log = []  # Clear log after saving\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJScI1Oy1fwA"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./urdu_model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-3,\n",
        "    fp16=True,\n",
        "    do_train=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_mis,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "callback = SaveModelCallback(repo_id=\"NeerjaK/NLP-Assignment2\")\n",
        "trainer.add_callback(callback)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JldvtOB71ttz"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCGX8-271y8K"
      },
      "outputs": [],
      "source": [
        "#### if model is partially trained then use this\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "repo_id = \"NeerjaK/Urdu_Model\"\n",
        "\n",
        "model_mis = AutoModelForSequenceClassification.from_pretrained(repo_id)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "\n",
        "trainer.train(resume_from_checkpoint=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UesyLCdh1-Q6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2391403-18a8-4ee6-a2da-00929e8a0b0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('trained_urdu_model/tokenizer_config.json',\n",
              " 'trained_urdu_model/special_tokens_map.json',\n",
              " 'trained_urdu_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "trainer.save_model(\"trained_urdu_model\")\n",
        "tokenizer.save_pretrained(\"trained_urdu_model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "qBgmWVdDplF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_input = \"خاتون کرسی سے گر کر جاں بحق\"\n",
        "input_dict = {'text': [custom_input]}\n",
        "input_dict = {'input_ids': [tokenizer.encode(custom_input)]}\n",
        "custom_dataset = Dataset.from_dict(input_dict)\n",
        "predictions = trainer.predict(custom_dataset)\n",
        "generated_outputs = predictions.predictions  # This will be logits\n",
        "output_ids = torch.argmax(torch.tensor(generated_outputs), dim=2)\n",
        "tokenizer.decode(output_ids[0])"
      ],
      "metadata": {
        "id": "w30nac0tagHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets"
      ],
      "metadata": {
        "id": "uS0GgEVVEYQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
        "from datasets import Dataset\n",
        "import torch"
      ],
      "metadata": {
        "id": "ZBTZLEhEEcN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Define the checkpoint directory\n",
        "checkpoint_path = \".\"  # Location where the model files are stored\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "\n",
        "# Load model (Generative Model)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint_path)\n",
        "\n",
        "# Prepare custom input for prediction\n",
        "custom_input = 'کرسی سے گر کر خاتون کی موت'\n",
        "\n",
        "# Tokenize the input\n",
        "input_ids = tokenizer.encode(custom_input, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text\n",
        "output_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,  # Maximum length of the generated sequence\n",
        "    num_beams=5,  # Beam search for diverse predictions\n",
        "    no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
        "    early_stopping=True  # Stop when all beams finish\n",
        ")\n",
        "\n",
        "# Decode the generated output\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the output\n",
        "print(\"Generated Text:\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqnNVWVYHiHZ",
        "outputId": "4ecd208a-fb95-45a2-dcb0-039a243a7099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: کرسی سے گر کر خاتون کی موت کے لیے میں ان کے گھر میں سے ایک میں  افراد ہلاک ہوئے تھے اور  رنز سے شکست حاصل کی گئی تھی کہ اس کی وجہ سے  لاکھ روپے سے بڑھ کر \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}